{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QspYXq2Y0-4a",
        "outputId": "d4838b85-5bd2-4113-ff8a-da39c709d4d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Installing dependencies...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.3/162.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDependencies installed\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"Installing dependencies...\")\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q transformers\n",
        "!pip install -q optimum\n",
        "!pip install -q onnx\n",
        "!pip install -q accelerate\n",
        "!pip install -q sentencepiece\n",
        "print(\"Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLYG2ETF7Cj0",
        "outputId": "e1d92756-de57-4ef8-ff6e-80cd3bf90c0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded modeling_phi3.py\n",
            "\n",
            " Files downloaded.\n"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "# import requests\n",
        "\n",
        "# MODEL_DIR = '/content/drive/MyDrive/phi3-merged'\n",
        "# base_url = \"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/raw/main/\"\n",
        "\n",
        "# files_to_download = [\n",
        "#     \"modeling_phi3.py\"\n",
        "# ]\n",
        "\n",
        "# for filename in files_to_download:\n",
        "#     try:\n",
        "#         url = base_url + filename\n",
        "#         response = requests.get(url)\n",
        "#         if response.status_code == 200:\n",
        "#             filepath = os.path.join(MODEL_DIR, filename)\n",
        "#             with open(filepath, 'wb') as f:\n",
        "#                 f.write(response.content)\n",
        "#             print(f\"Downloaded {filename}\")\n",
        "#         else:\n",
        "#             print(f\"Could not download {filename}\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error downloading {filename}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6y2ffPP45XQ",
        "outputId": "5ae82c71-2557-4a02-e470-25a2b8a22234"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (0.22.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Converting Phi-3 to ONNX using onnxruntime-genai builder...\n",
            "Input: /content/drive/MyDrive/phi3-merged\n",
            "Output: /content/drive/MyDrive/phi3-onnx-output\n",
            "\n",
            "2025-11-27 11:39:09,631 numexpr.utils [INFO] - NumExpr defaulting to 12 threads.\n",
            "Valid precision + execution provider combinations are: FP32 CPU, FP32 CUDA, FP16 CUDA, FP16 DML, BF16 CUDA, FP16 TRT-RTX, INT4 CPU, INT4 CUDA, INT4 DML, INT4 WebGPU\n",
            "Extra options: {'trust_remote_code': 'True'}\n",
            "GroupQueryAttention (GQA) is used in this model.\n",
            "2025-11-27 11:39:13.319813: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-27 11:39:13.337550: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764243553.359703    1971 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764243553.366358    1971 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764243553.383276    1971 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764243553.383308    1971 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764243553.383311    1971 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764243553.383314    1971 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-27 11:39:13.388281: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-11-27 11:39:19,459 transformers_modules.phi3_hyphen_merged.modeling_phi3 [WARNING] - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "2025-11-27 11:39:19,459 transformers_modules.phi3_hyphen_merged.modeling_phi3 [WARNING] - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "Loading checkpoint shards: 100% 2/2 [02:09<00:00, 64.93s/it]\n",
            "Reading embedding layer\n",
            "Reading decoder layer 0\n",
            "Reading decoder layer 1\n",
            "Reading decoder layer 2\n",
            "Reading decoder layer 3\n",
            "Reading decoder layer 4\n",
            "Reading decoder layer 5\n",
            "Reading decoder layer 6\n",
            "Reading decoder layer 7\n",
            "Reading decoder layer 8\n",
            "Reading decoder layer 9\n",
            "Reading decoder layer 10\n",
            "Reading decoder layer 11\n",
            "Reading decoder layer 12\n",
            "Reading decoder layer 13\n",
            "Reading decoder layer 14\n",
            "Reading decoder layer 15\n",
            "Reading decoder layer 16\n",
            "Reading decoder layer 17\n",
            "Reading decoder layer 18\n",
            "Reading decoder layer 19\n",
            "Reading decoder layer 20\n",
            "Reading decoder layer 21\n",
            "Reading decoder layer 22\n",
            "Reading decoder layer 23\n",
            "Reading decoder layer 24\n",
            "Reading decoder layer 25\n",
            "Reading decoder layer 26\n",
            "Reading decoder layer 27\n",
            "Reading decoder layer 28\n",
            "Reading decoder layer 29\n",
            "Reading decoder layer 30\n",
            "Reading decoder layer 31\n",
            "Reading final norm\n",
            "Reading LM head\n",
            "Saving ONNX model in /content/drive/MyDrive/phi3-onnx-output\n",
            "2025-11-27 11:42:09,705 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/constant_nodes/INT64/[1] ...\n",
            "2025-11-27 11:42:09,706 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/attn_mask_reformat/attn_mask_subgraph/ReduceSum ...\n",
            "2025-11-27 11:42:09,706 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/attn_mask_reformat/attn_mask_subgraph/Sub ...\n",
            "2025-11-27 11:42:09,706 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast ...\n",
            "2025-11-27 11:42:09,706 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/attn_mask_reformat/attn_mask_subgraph/Shape ...\n",
            "2025-11-27 11:42:09,706 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/constant_nodes/INT64/1 ...\n",
            "2025-11-27 11:42:09,706 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/attn_mask_reformat/attn_mask_subgraph/Gather ...\n",
            "2025-11-27 11:42:09,706 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast ...\n",
            "2025-11-27 11:42:09,706 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/embed_tokens/Gather ...\n",
            "2025-11-27 11:42:09,706 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/input_layernorm/LayerNorm ...\n",
            "2025-11-27 11:42:09,706 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:09,887 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:09,887 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:09,887 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:09,939 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:09,939 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:09,939 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:10,108 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:10,108 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:10,315 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:10,315 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:10,315 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:10,315 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/mlp/Mul ...\n",
            "2025-11-27 11:42:10,315 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:10,478 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:10,478 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:10,478 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:10,663 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:10,663 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:10,663 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:10,716 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:10,716 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:10,716 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:10,879 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:10,879 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:11,050 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:11,051 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:11,051 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:11,051 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/mlp/Mul ...\n",
            "2025-11-27 11:42:11,051 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:11,218 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:11,218 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:11,218 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:11,402 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:11,402 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:11,402 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:11,459 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:11,459 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:11,459 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:11,624 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:11,624 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:11,817 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:11,818 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:11,818 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:11,818 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/mlp/Mul ...\n",
            "2025-11-27 11:42:11,818 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:11,980 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:11,981 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:11,981 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:12,168 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:12,168 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:12,168 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:12,225 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:12,225 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:12,225 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:12,388 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:12,388 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:12,562 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:12,563 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:12,563 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:12,563 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/mlp/Mul ...\n",
            "2025-11-27 11:42:12,563 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:12,726 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:12,726 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:12,726 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:12,909 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:12,909 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:12,909 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:12,966 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:12,966 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:12,966 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:13,130 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:13,130 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:13,320 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:13,320 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:13,320 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:13,320 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/mlp/Mul ...\n",
            "2025-11-27 11:42:13,320 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:13,483 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:13,483 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:13,483 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:13,668 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:13,668 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:13,669 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:13,721 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:13,721 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:13,721 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:13,885 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:13,885 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:14,057 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:14,057 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:14,057 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:14,058 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/mlp/Mul ...\n",
            "2025-11-27 11:42:14,058 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:14,221 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:14,221 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:14,221 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:14,413 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:14,414 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:14,414 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:14,479 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:14,479 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:14,479 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:14,647 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:14,647 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:14,822 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:14,822 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:14,822 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:14,822 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/mlp/Mul ...\n",
            "2025-11-27 11:42:14,822 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:15,059 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:15,059 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:15,060 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:15,315 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:15,315 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:15,316 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:15,396 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:15,396 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:15,396 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:15,632 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:15,632 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:15,871 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:15,871 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:15,871 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:15,871 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/mlp/Mul ...\n",
            "2025-11-27 11:42:15,871 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:16,105 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:16,105 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:16,105 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:16,353 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:16,353 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:16,353 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:16,438 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:16,438 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:16,438 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:16,671 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:16,671 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:16,863 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:16,863 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:16,863 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:16,863 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/mlp/Mul ...\n",
            "2025-11-27 11:42:16,863 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:17,093 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:17,093 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:17,093 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:17,279 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:17,279 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:17,279 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:17,332 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:17,332 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:17,332 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:17,506 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:17,506 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:17,691 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:17,691 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:17,691 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:17,691 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/mlp/Mul ...\n",
            "2025-11-27 11:42:17,691 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:17,855 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:17,855 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:17,855 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:18,040 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:18,040 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:18,040 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:18,096 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:18,097 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:18,097 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:18,264 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:18,264 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:18,438 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:18,438 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:18,438 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:18,438 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/mlp/Mul ...\n",
            "2025-11-27 11:42:18,438 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:18,602 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:18,603 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:18,603 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:18,790 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:18,790 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:18,790 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:18,842 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:18,843 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:18,843 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:18,995 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:18,996 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:19,152 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:19,152 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:19,152 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:19,152 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/mlp/Mul ...\n",
            "2025-11-27 11:42:19,152 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:19,298 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:19,298 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:19,298 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:19,481 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:19,481 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:19,481 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:19,534 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:19,534 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:19,534 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:19,688 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:19,689 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:19,844 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:19,845 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:19,845 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:19,845 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/mlp/Mul ...\n",
            "2025-11-27 11:42:19,845 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:19,992 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:19,992 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:19,992 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:20,174 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:20,174 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:20,174 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:20,227 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:20,227 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:20,227 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:20,381 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:20,381 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:20,545 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:20,545 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:20,545 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:20,545 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/mlp/Mul ...\n",
            "2025-11-27 11:42:20,545 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:20,692 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:20,692 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:20,692 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:20,873 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:20,873 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:20,873 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:20,925 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:20,925 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:20,925 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:21,079 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:21,079 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:21,236 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:21,236 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:21,236 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:21,236 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/mlp/Mul ...\n",
            "2025-11-27 11:42:21,236 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:21,383 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:21,383 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:21,383 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:21,575 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:21,575 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:21,575 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:21,628 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:21,629 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:21,629 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:21,781 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:21,781 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:21,935 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:21,936 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:21,936 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:21,936 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/mlp/Mul ...\n",
            "2025-11-27 11:42:21,936 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:22,081 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:22,081 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:22,081 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:22,263 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:22,263 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:22,263 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:22,317 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:22,317 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:22,317 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:22,469 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:22,469 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:22,629 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:22,629 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:22,629 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:22,629 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/mlp/Mul ...\n",
            "2025-11-27 11:42:22,629 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:22,776 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:22,776 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:22,776 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:22,958 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:22,958 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:22,958 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:23,011 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:23,011 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:23,011 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:23,163 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:23,163 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:23,320 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:23,320 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:23,320 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:23,320 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/mlp/Mul ...\n",
            "2025-11-27 11:42:23,320 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:23,466 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:23,466 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:23,466 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:23,653 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:23,653 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:23,653 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:23,706 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:23,706 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:23,707 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:23,860 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:23,860 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:24,016 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:24,016 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:24,016 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:24,016 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/mlp/Mul ...\n",
            "2025-11-27 11:42:24,016 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:24,162 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:24,163 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:24,163 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:24,345 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:24,345 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:24,345 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:24,397 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:24,397 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:24,397 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:24,550 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:24,551 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:24,714 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:24,714 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:24,714 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:24,714 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/mlp/Mul ...\n",
            "2025-11-27 11:42:24,714 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:24,860 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:24,861 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:24,861 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:25,041 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:25,042 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:25,042 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:25,094 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:25,094 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:25,094 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:25,248 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:25,248 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:25,401 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:25,401 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:25,401 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:25,401 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/mlp/Mul ...\n",
            "2025-11-27 11:42:25,401 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:25,548 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:25,549 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:25,549 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:25,734 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:25,735 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:25,735 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:25,787 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:25,788 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:25,788 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:25,959 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:25,959 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:26,113 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:26,113 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:26,113 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:26,114 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/mlp/Mul ...\n",
            "2025-11-27 11:42:26,114 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:26,261 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:26,261 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:26,261 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:26,442 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:26,443 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:26,443 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:26,495 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:26,495 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:26,495 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:26,648 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:26,648 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:26,808 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:26,808 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:26,808 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:26,808 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/mlp/Mul ...\n",
            "2025-11-27 11:42:26,808 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:26,954 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:26,954 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:26,954 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:27,135 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:27,135 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:27,135 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:27,215 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:27,216 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:27,216 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:27,437 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:27,437 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:27,672 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:27,672 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:27,672 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:27,672 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/mlp/Mul ...\n",
            "2025-11-27 11:42:27,672 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:27,892 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:27,892 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.24/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:27,892 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.24/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:28,142 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.24/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:28,143 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.24/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:28,143 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.24/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:28,223 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.24/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:28,223 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.24/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:28,223 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.24/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:28,450 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.24/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:28,450 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.24/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:28,673 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.24/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:28,673 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.24/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:28,673 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.24/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:28,673 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.24/mlp/Mul ...\n",
            "2025-11-27 11:42:28,673 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.24/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:28,843 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.24/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:28,843 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.25/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:28,843 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.25/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:29,106 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.25/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:29,106 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.25/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:29,107 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.25/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:29,186 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.25/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:29,187 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.25/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:29,187 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.25/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:29,399 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.25/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:29,399 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.25/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:29,555 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.25/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:29,556 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.25/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:29,556 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.25/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:29,556 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.25/mlp/Mul ...\n",
            "2025-11-27 11:42:29,556 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.25/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:29,701 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.25/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:29,701 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.26/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:29,701 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.26/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:29,890 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.26/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:29,891 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.26/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:29,891 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.26/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:29,943 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.26/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:29,943 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.26/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:29,943 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.26/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:30,097 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.26/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:30,097 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.26/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:30,252 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.26/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:30,252 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.26/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:30,252 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.26/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:30,252 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.26/mlp/Mul ...\n",
            "2025-11-27 11:42:30,252 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.26/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:30,398 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.26/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:30,398 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.27/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:30,398 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.27/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:30,579 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.27/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:30,580 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.27/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:30,580 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.27/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:30,632 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.27/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:30,632 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.27/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:30,632 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.27/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:30,784 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.27/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:30,784 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.27/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:30,952 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.27/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:30,952 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.27/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:30,953 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.27/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:30,953 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.27/mlp/Mul ...\n",
            "2025-11-27 11:42:30,953 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.27/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:31,170 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.27/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:31,170 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.28/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:31,170 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.28/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:31,428 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.28/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:31,428 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.28/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:31,428 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.28/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:31,508 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.28/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:31,508 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.28/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:31,508 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.28/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:31,738 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.28/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:31,738 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.28/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:31,961 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.28/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:31,961 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.28/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:31,962 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.28/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:31,962 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.28/mlp/Mul ...\n",
            "2025-11-27 11:42:31,962 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.28/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:32,177 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.28/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:32,178 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.29/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:32,178 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.29/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:32,436 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.29/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:32,436 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.29/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:32,436 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.29/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:32,517 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.29/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:32,517 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.29/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:32,517 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.29/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:32,740 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.29/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:32,740 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.29/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:32,963 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.29/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:32,963 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.29/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:32,964 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.29/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:32,964 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.29/mlp/Mul ...\n",
            "2025-11-27 11:42:32,964 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.29/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:33,183 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.29/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:33,183 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.30/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:33,183 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.30/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:33,442 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.30/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:33,442 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.30/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:33,442 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.30/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:33,522 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.30/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:33,522 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.30/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:33,522 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.30/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:33,744 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.30/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:33,745 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.30/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:33,973 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.30/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:33,973 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.30/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:33,973 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.30/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:33,973 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.30/mlp/Mul ...\n",
            "2025-11-27 11:42:33,973 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.30/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:34,193 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.30/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:34,193 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.31/input_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:34,193 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.31/attn/qkv_proj/MatMul ...\n",
            "2025-11-27 11:42:34,458 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.31/attn/qkv_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:34,458 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.31/attn/GroupQueryAttention ...\n",
            "2025-11-27 11:42:34,458 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.31/attn/o_proj/MatMul ...\n",
            "2025-11-27 11:42:34,539 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.31/attn/o_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:34,539 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.31/post_attention_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:34,539 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.31/mlp/gate_proj/MatMul ...\n",
            "2025-11-27 11:42:34,761 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.31/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:34,761 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.31/mlp/up_proj/MatMul ...\n",
            "2025-11-27 11:42:34,990 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.31/mlp/up_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:34,990 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.31/mlp/act_fn/Sigmoid ...\n",
            "2025-11-27 11:42:34,990 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.31/mlp/act_fn/Mul ...\n",
            "2025-11-27 11:42:34,990 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.31/mlp/Mul ...\n",
            "2025-11-27 11:42:34,990 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.31/mlp/down_proj/MatMul ...\n",
            "2025-11-27 11:42:35,210 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.31/mlp/down_proj/MatMul with 4 bits ...\n",
            "2025-11-27 11:42:35,210 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.32/final_norm_layernorm/SkipLayerNorm ...\n",
            "2025-11-27 11:42:35,210 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /lm_head/MatMul ...\n",
            "2025-11-27 11:42:36,026 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /lm_head/MatMul with 4 bits ...\n",
            "Saving model.embed_tokens.weight (f16, [32064,3072]): 100% 390/390 [00:06<00:00, 61.75it/s]\n",
            "Saving GenAI config in /content/drive/MyDrive/phi3-onnx-output\n",
            "Saving processing files in /content/drive/MyDrive/phi3-onnx-output for GenAI\n",
            "\n",
            "Checking output files...\n",
            "✅ SUCCESS! Created 2 ONNX related files.\n",
            "  - model.onnx.data: 2185.94 MB\n",
            "  - model.onnx: 0.22 MB\n"
          ]
        }
      ],
      "source": [
        "!pip install -q onnxruntime-genai onnx_ir\n",
        "!pip install --upgrade transformers tokenizers safetensors\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "MODEL_DIR = '/content/drive/MyDrive/phi3-merged'\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/phi3-onnx-output'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Converting Phi-3 to ONNX using onnxruntime-genai builder...\")\n",
        "print(f\"Input: {MODEL_DIR}\")\n",
        "print(f\"Output: {OUTPUT_DIR}\\n\")\n",
        "\n",
        "!python -m onnxruntime_genai.models.builder \\\n",
        "    -m \"{MODEL_DIR}\" \\\n",
        "    -o \"{OUTPUT_DIR}\" \\\n",
        "    -p int4 \\\n",
        "    -e cuda \\\n",
        "    --extra_options trust_remote_code=True\n",
        "\n",
        "print(\"\\nChecking output files...\")\n",
        "onnx_files = glob.glob(f\"{OUTPUT_DIR}/**/*.onnx*\", recursive=True)\n",
        "if onnx_files:\n",
        "    print(f\"✅ SUCCESS! Created {len(onnx_files)} ONNX related files.\")\n",
        "    for f in onnx_files:\n",
        "        size_mb = os.path.getsize(f) / (1024**2)\n",
        "        print(f\"  - {os.path.basename(f)}: {size_mb:.2f} MB\")\n",
        "else:\n",
        "    print(\"❌ No ONNX files found. Conversion might have failed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-r1KV1V6Xc0",
        "outputId": "985f4668-081f-43e6-9017-8a3560f340b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  - added_tokens.json\n",
            "  - chat_template.jinja\n",
            "  - genai_config.json\n",
            "  - model.onnx\n",
            "  - model.onnx.data\n",
            "  - special_tokens_map.json\n",
            "  - tokenizer.json\n",
            "  - tokenizer.model\n",
            "  - tokenizer_config.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "files = os.listdir(OUTPUT_DIR)\n",
        "for f in sorted(files):\n",
        "    print(f\"  - {f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "1Bm4M5lfKJr9",
        "outputId": "4ae29307-f181-4c49-dd06-40b51b9448a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: onnxruntime 1.23.2\n",
            "Uninstalling onnxruntime-1.23.2:\n",
            "  Successfully uninstalled onnxruntime-1.23.2\n",
            "Found existing installation: onnxruntime-genai 0.11.2\n",
            "Uninstalling onnxruntime-genai-0.11.2:\n",
            "  Successfully uninstalled onnxruntime-genai-0.11.2\n",
            "Collecting onnxruntime-genai-cuda\n",
            "  Downloading onnxruntime_genai_cuda-0.11.2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (676 bytes)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.12/dist-packages (from onnxruntime-genai-cuda) (2.0.2)\n",
            "Collecting onnxruntime-gpu>=1.23.2 (from onnxruntime-genai-cuda)\n",
            "  Downloading onnxruntime_gpu-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu>=1.23.2->onnxruntime-genai-cuda) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu>=1.23.2->onnxruntime-genai-cuda) (25.9.23)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu>=1.23.2->onnxruntime-genai-cuda) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu>=1.23.2->onnxruntime-genai-cuda) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu>=1.23.2->onnxruntime-genai-cuda) (1.14.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime-gpu>=1.23.2->onnxruntime-genai-cuda) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime-gpu>=1.23.2->onnxruntime-genai-cuda) (1.3.0)\n",
            "Downloading onnxruntime_genai_cuda-0.11.2-cp312-cp312-manylinux_2_28_x86_64.whl (88.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.1/88.1 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime_gpu-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (300.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.5/300.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnxruntime-gpu, onnxruntime-genai-cuda\n",
            "Successfully installed onnxruntime-genai-cuda-0.11.2 onnxruntime-gpu-1.23.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "cca375f0a4bc4bc3b82691c0fb63d763",
              "pip_warning": {
                "packages": [
                  "onnxruntime_genai"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip uninstall -y onnxruntime onnxruntime-genai\n",
        "!pip install onnxruntime-genai-cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiPufzhr1gNm",
        "outputId": "788cbe07-4e67-4860-efff-0aee5bd89137"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "Test Causal LM (Phi-3) Inference for Summarization\n",
            "======================================================================\n",
            "Loading ONNX GenAI model...\n",
            "Generating...\n",
            "\n",
            "Summarization successful!\n",
            "Original Text Length: 448 chars\n",
            "Summary:\n",
            "Evaluation of customer support operations reveals inconsistent issue resolution rates across regions.\n",
            "Discrepancies attributed to uneven staffing and varying degrees of product knowledge.\n",
            "Leadership considering revised training program and standardized workflow guidelines .\n"
          ]
        }
      ],
      "source": [
        "import traceback\n",
        "import onnxruntime_genai as og\n",
        "import numpy as np\n",
        "\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/phi3-onnx-output'\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Test Causal LM (Phi-3) Inference for Summarization\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "text_to_summarize = (\n",
        "    \"A recent evaluation of the company’s customer support operations found that while response times have improved significantly, issue resolution rates remain inconsistent across regions. The report attributes these discrepancies to uneven staffing levels and varying degrees of product knowledge among support teams. Leadership is considering a revised training program and standardized workflow guidelines to enhance consistency and service quality.\"\n",
        ")\n",
        "\n",
        "formatted_prompt = f\"Summarize the following article concisely: {text_to_summarize}\"\n",
        "prompt = f\"<|user|>\\n{formatted_prompt}<|end|>\\n<|assistant|>\\n\"\n",
        "\n",
        "try:\n",
        "    print(\"Loading ONNX GenAI model...\")\n",
        "    model = og.Model(OUTPUT_DIR)\n",
        "    tokenizer = og.Tokenizer(model)\n",
        "    input_tokens = tokenizer.encode(prompt)\n",
        "    input_tokens_np = np.array(input_tokens, dtype=np.int32)\n",
        "    prompt_length = len(input_tokens)\n",
        "    params = og.GeneratorParams(model)\n",
        "    params.set_search_options(\n",
        "        max_length=2048,\n",
        "        num_beams=4,\n",
        "        do_sample=False,\n",
        "    )\n",
        "    generator = og.Generator(model, params)\n",
        "    generator.append_tokens(input_tokens_np)\n",
        "\n",
        "    print(\"Generating...\")\n",
        "    while not generator.is_done():\n",
        "        generator.generate_next_token()\n",
        "    all_tokens = generator.get_sequence(0)\n",
        "    new_tokens = all_tokens[prompt_length:]\n",
        "    summary_text = tokenizer.decode(new_tokens)\n",
        "\n",
        "    print(f\"\\nSummarization successful!\")\n",
        "    print(f\"Original Text Length: {len(text_to_summarize)} chars\")\n",
        "    print(f\"Summary:\\n{summary_text.strip()}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Test failed: {e}\")\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Vd8IV0sKq3v"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gQcBF7V-KTB",
        "outputId": "e8df423d-b07a-4652-f8e3-a78c72979510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "  adding: content/drive/MyDrive/phi3-onnx-output/ (stored 0%)\n",
            "  adding: content/drive/MyDrive/phi3-onnx-output/model.onnx.data"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!zip -r /content/drive/MyDrive/phi3_mini_4k_onnx_q4.zip /content/drive/MyDrive/phi3-onnx-output"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}